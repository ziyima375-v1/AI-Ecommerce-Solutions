[下载对外展示 README（deduplication_engine.py）](sandbox:/mnt/data/README_deduplication_engine.md)

````md
# 图片去重比对工具（deduplication_engine.py）

本工具是一个本地桌面程序，用于将“新图片文件夹”中的图片与“主库文件夹（数据库）”中的图片进行相似度比对：
- 判定为重复：按设置进行处理（推荐“移到隔离区”）
- 判定为不重复：复制到主库并写入索引，后续可持续增量比对

适用场景：素材库去重、上新前批量排重、历史图库增量维护。

---

## 1. 运行环境

- Windows / macOS / Linux（更推荐 Windows）
- Python 3.10+（推荐 3.11）

---

## 2. 安装依赖

建议使用虚拟环境（可选但推荐）：

```bash
python -m venv .venv
# Windows
.venv\Scripts\activate
# macOS/Linux
source .venv/bin/activate
````

安装依赖：

```bash
pip install pillow imagehash numpy scikit-image
```

> 说明：如果你后续要把结果导出或扩展功能，可再按需安装其他库；本程序运行通常只需要以上依赖。

---

## 3. 启动程序

在脚本所在目录执行：

```bash
python deduplication_engine.py
```

启动后会打开桌面窗口。

---

## 4. 使用流程（GUI）

### 4.1 选择文件夹

1. **主库文件夹（数据库）**

* 这是你的“已收录素材库”所在目录
* 程序会在该目录内维护一个索引数据库文件（用于加速下次比对）

2. **新图片文件夹**

* 放入你要检查是否重复的一批图片（可包含子文件夹）

支持的图片格式：`.jpg .jpeg .png .bmp .webp .tif .tiff`

---

### 4.2 参数设置（建议默认即可）

* **哈希汉明距离阈值**（1~10）

  * 越小：越严格（误判少，但可能漏判）
  * 越大：越宽松（重复抓得多，但可能误判）
  * 推荐：`5`（默认）

* **启用 SSIM 精核**（更准，稍慢）

  * 推荐开启（默认开启）
  * **SSIM 阈值**推荐：`0.85`（默认）

---

### 4.3 选择“重复处理方式”（强烈建议选择隔离）

* **移到隔离区（推荐）**

  * 判定重复的图片不会删除，而是移动到：
    `新图片文件夹/Quarantine/`
  * 便于人工复核，避免误删

* **直接删除（不可恢复）**

  * 判定重复后立即删除文件
  * 程序会在开始前弹出二次确认

---

### 4.4 开始处理

点击 **开始比对与处理** 后，程序会按以下顺序执行：

1. 建立/更新主库索引（增量更新，不会每次全量重建）
2. 扫描新图片文件夹
3. 逐张比对：重复 → 按规则处理；不重复 → 复制进主库并写入索引
4. 在界面日志区输出进度与结果统计

---

## 5. 输出与生成文件说明

* 主库文件夹内会生成索引数据库文件（用于加速下次比对）：

  * `imgdedup.sqlite`

* 若选择“移到隔离区”，新图片文件夹内会生成：

  * `Quarantine/`（隔离目录）

---

## 6. 常见问题

### 6.1 运行报错：缺少模块 / ImportError

按提示安装依赖即可：

```bash
pip install pillow imagehash numpy scikit-image
```

### 6.2 速度慢怎么办？

* 关闭占用 CPU/磁盘的程序（例如同步盘、杀毒全盘扫描）
* 新图片文件夹尽量放在 SSD
* 参数保持默认（SSIM 会更慢但更准）

### 6.3 误判怎么办？

* 优先使用“移到隔离区”，人工抽查后再决定是否删除
* 将“哈希阈值”调小一些（更严格）
* 确保启用 SSIM 精核，并适当调高 SSIM 阈值（例如 0.88~0.92）

---

## 7. 入口文件

* 主入口：`deduplication_engine.py`

```
```
